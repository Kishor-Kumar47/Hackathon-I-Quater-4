"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[3461],{1185:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"week-1-2-introduction","title":"Week 1-2: Physical AI Introduction - Foundations & Embodied Intelligence, Sensor Systems","description":"1. Introduction to Physical AI","source":"@site/docs/week-1-2-introduction.md","sourceDirName":".","slug":"/week-1-2-introduction","permalink":"/Hackathon-I-Quater-4/docs/week-1-2-introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/Kishor-Kumar47/Hackathon-I-Quater-4/tree/main/my-website/docs/week-1-2-introduction.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"sidebar_label":"Week 1-2: Physical AI Introduction"},"sidebar":"tutorialSidebar","previous":{"title":"Translate your site","permalink":"/Hackathon-I-Quater-4/docs/tutorial-extras/translate-your-site"},"next":{"title":"Week 3-5: ROS 2 Fundamentals","permalink":"/Hackathon-I-Quater-4/docs/week-3-5-ros2-fundamentals"}}');var t=i(4848),a=i(8453);const r={sidebar_position:3,sidebar_label:"Week 1-2: Physical AI Introduction"},o="Week 1-2: Physical AI Introduction - Foundations & Embodied Intelligence, Sensor Systems",l={},c=[{value:"1. Introduction to Physical AI",id:"1-introduction-to-physical-ai",level:2},{value:"2. Foundations and Embodied Intelligence",id:"2-foundations-and-embodied-intelligence",level:2},{value:"2.1 What is Physical AI?",id:"21-what-is-physical-ai",level:3},{value:"2.2 Embodied Intelligence",id:"22-embodied-intelligence",level:3},{value:"3. Sensor Systems for Physical AI",id:"3-sensor-systems-for-physical-ai",level:2},{value:"3.1 Types of Sensors",id:"31-types-of-sensors",level:3},{value:"3.2 Sensor Data Processing Challenges",id:"32-sensor-data-processing-challenges",level:3},{value:"4. Python Code Examples for Sensor Systems",id:"4-python-code-examples-for-sensor-systems",level:2},{value:"Example 1: Simulating LiDAR Data Processing",id:"example-1-simulating-lidar-data-processing",level:3},{value:"Example 2: Processing Simulated IMU Data for Orientation Estimation",id:"example-2-processing-simulated-imu-data-for-orientation-estimation",level:3},{value:"5. Exercises",id:"5-exercises",level:2},{value:"6. Short Quiz",id:"6-short-quiz",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"week-1-2-physical-ai-introduction---foundations--embodied-intelligence-sensor-systems",children:"Week 1-2: Physical AI Introduction - Foundations & Embodied Intelligence, Sensor Systems"})}),"\n",(0,t.jsx)(n.h2,{id:"1-introduction-to-physical-ai",children:"1. Introduction to Physical AI"}),"\n",(0,t.jsx)(n.p,{children:"Physical AI is an interdisciplinary field that combines artificial intelligence with robotics and physical systems. Unlike purely software-based AI, Physical AI agents interact with the real world through sensors and actuators, performing tasks that require physical embodiment and interaction. This chapter introduces the foundational concepts of Physical AI, explores the significance of embodied intelligence, and delves into the essential sensor systems that enable AI agents to perceive their environment."}),"\n",(0,t.jsx)(n.h2,{id:"2-foundations-and-embodied-intelligence",children:"2. Foundations and Embodied Intelligence"}),"\n",(0,t.jsx)(n.h3,{id:"21-what-is-physical-ai",children:"2.1 What is Physical AI?"}),"\n",(0,t.jsx)(n.p,{children:"Physical AI focuses on creating intelligent agents that operate in the physical world. This involves integrating AI algorithms with robotic platforms, allowing them to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perceive:"})," Gather information about their surroundings using various sensors."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reason:"})," Process sensory data, make decisions, and plan actions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Act:"})," Execute physical actions through actuators (e.g., motors, grippers)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learn:"})," Adapt and improve their performance over time through experience."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"22-embodied-intelligence",children:"2.2 Embodied Intelligence"}),"\n",(0,t.jsx)(n.p,{children:"Embodied intelligence posits that an agent's intelligence is deeply intertwined with its physical body and its interactions with the environment. Key aspects include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Situatedness:"})," Agents operate within a specific context and environment, influencing their perceptions and actions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Interaction:"})," Continuous feedback loops between the agent's body, its actions, and the environment shape its learning and behavior."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Morphological Computation:"})," The physical design and properties of the body can offload computational burden from the brain, simplifying control and enabling emergent behaviors. For example, a robot with compliant joints might naturally absorb shocks without complex control algorithms."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why is embodied intelligence important?"}),"\nTraditional AI often separates mind from body. However, embodied AI argues that physical interaction provides crucial insights and constraints that are difficult to simulate or achieve through purely abstract reasoning. It allows for:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness:"})," Handling real-world uncertainties and variations more effectively."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Efficiency:"})," Leveraging physical properties to simplify tasks."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Understanding:"})," Developing a more intuitive grasp of physical laws and object properties."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"3-sensor-systems-for-physical-ai",children:"3. Sensor Systems for Physical AI"}),"\n",(0,t.jsx)(n.p,{children:'Sensors are the "eyes, ears, and touch" of a Physical AI agent, providing the necessary data to understand its environment. Without accurate and timely sensory information, an intelligent agent cannot effectively perceive or interact with the physical world.'}),"\n",(0,t.jsx)(n.h3,{id:"31-types-of-sensors",children:"3.1 Types of Sensors"}),"\n",(0,t.jsx)(n.p,{children:"Physical AI systems utilize a wide array of sensors, each with its strengths and applications:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision Sensors (Cameras):"})," Provide visual information, enabling object detection, recognition, tracking, and scene understanding."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monocular Cameras:"})," Single camera, provides 2D images. Depth estimation requires complex algorithms."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stereo Cameras:"})," Two cameras separated by a baseline, mimicking human vision. Used for accurate 3D depth perception."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RGB-D Cameras (e.g., Intel RealSense, Microsoft Kinect):"})," Provide both color (RGB) and depth (D) information directly, often using infrared projectors."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Distance/Range Sensors (LiDAR, Ultrasonic, Infrared):"})," Measure the distance to objects."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LiDAR (Light Detection and Ranging):"})," Uses pulsed laser light to measure distances, generating precise 3D point clouds of the environment. Excellent for mapping and navigation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ultrasonic Sensors:"})," Emit sound waves and measure the time it takes for the echo to return. Good for close-range obstacle detection."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Infrared (IR) Sensors:"})," Emit infrared light and detect reflections. Can measure distance but are sensitive to ambient light and object color."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Inertial Measurement Units (IMUs):"})," Measure an agent's orientation, angular velocity, and linear acceleration."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accelerometers:"})," Measure linear acceleration (changes in velocity)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gyroscopes:"})," Measure angular velocity (rotational speed)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Magnetometers:"})," Measure magnetic fields, often used to determine heading relative to magnetic North.\nIMUs are crucial for estimating position, maintaining balance, and controlling movement."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Proprioceptive Sensors:"})," Provide information about the agent's own state (e.g., joint angles, motor speeds, force exerted by grippers)."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"32-sensor-data-processing-challenges",children:"3.2 Sensor Data Processing Challenges"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Noise:"})," Raw sensor data is often noisy and requires filtering."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Calibration:"})," Sensors need to be accurately calibrated to provide reliable measurements."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Synchronization:"})," Data from multiple sensors must be synchronized for accurate fusion."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Volume:"})," High-resolution sensors like LiDAR and cameras generate massive amounts of data, requiring efficient processing."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"4-python-code-examples-for-sensor-systems",children:"4. Python Code Examples for Sensor Systems"}),"\n",(0,t.jsx)(n.p,{children:"Here, we'll provide simplified Python examples to illustrate how to interface with and process data from common sensor types. These examples assume basic libraries and conceptual understanding."}),"\n",(0,t.jsx)(n.h3,{id:"example-1-simulating-lidar-data-processing",children:"Example 1: Simulating LiDAR Data Processing"}),"\n",(0,t.jsx)(n.p,{children:"This example simulates receiving LiDAR scan data and processing it to detect obstacles. In a real-world scenario, you would interface with a LiDAR sensor's SDK."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport math\n\nclass SimulatedLiDAR:\n    def __init__(self, num_points=360, max_range=10.0, min_angle=-180, max_angle=180):\n        self.num_points = num_points\n        self.max_range = max_range\n        self.angles = np.linspace(math.radians(min_angle), math.radians(max_angle), num_points)\n\n    def get_scan(self, obstacles=None):\n        """Simulates a LiDAR scan, including some noise and \'obstacles\'."""\n        scan_data = np.full(self.num_points, self.max_range) # Initialize all points to max_range\n\n        if obstacles:\n            for i, angle in enumerate(self.angles):\n                for obs_center, obs_radius in obstacles:\n                    # Simple obstacle model: if ray intersects a circle\n                    ox, oy = obs_center\n                    px, py = 0, 0 # LiDAR at origin\n\n                    # Vector from LiDAR to obstacle center\n                    cx, cy = ox - px, oy - py\n\n                    # Ray direction vector\n                    vx, vy = math.cos(angle), math.sin(angle)\n\n                    # Project obstacle center onto ray\n                    t = (cx * vx + cy * vy)\n\n                    # Closest point on ray to obstacle center\n                    closest_x, closest_y = px + t * vx, py + t * vy\n\n                    # Distance from closest point on ray to obstacle center\n                    dist_to_center = math.sqrt((closest_x - ox)**2 + (closest_y - oy)**2)\n\n                    if dist_to_center < obs_radius:\n                        # If ray intersects obstacle, calculate intersection point distance\n                        # For simplicity, just use the distance to the obstacle center if it\'s in front of the LiDAR\n                        # and within the ray\'s general direction\n                        if t > 0: # Obstacle must be in front of the LiDAR\n                            dist_to_obs = math.sqrt(ox**2 + oy**2) # Distance from origin to obstacle center\n                            if dist_to_obs < scan_data[i]:\n                                scan_data[i] = min(dist_to_obs, self.max_range) # Update with obstacle distance\n\n        # Add some random noise\n        noise = np.random.normal(0, 0.05, self.num_points)\n        scan_data = np.clip(scan_data + noise, 0, self.max_range)\n\n        return scan_data, self.angles\n\n# --- Usage ---\nif __name__ == "__main__":\n    lidar = SimulatedLiDAR()\n\n    # Define some simulated obstacles: (center_x, center_y), radius\n    obstacles = [((2.0, 1.0), 0.5), ((-1.5, 3.0), 1.0)]\n\n    scan_data, angles = lidar.get_scan(obstacles)\n\n    print("Simulated LiDAR Scan (first 10 points):")\n    for i in range(10):\n        print(f"Angle: {math.degrees(angles[i]):.2f}\xb0, Distance: {scan_data[i]:.2f}m")\n\n    # Simple obstacle detection: find points below a threshold\n    obstacle_threshold = 1.0 # meters\n    close_points_indices = np.where(scan_data < obstacle_threshold)[0]\n\n    if len(close_points_indices) > 0:\n        print(f"\nObstacles detected within {obstacle_threshold}m at angles (degrees):")\n        for idx in close_points_indices:\n            print(f"- {math.degrees(angles[idx]):.2f}\xb0")\n    else:\n        print(f"\nNo obstacles detected within {obstacle_threshold}m.")\n\n'})}),"\n",(0,t.jsx)(n.h3,{id:"example-2-processing-simulated-imu-data-for-orientation-estimation",children:"Example 2: Processing Simulated IMU Data for Orientation Estimation"}),"\n",(0,t.jsx)(n.p,{children:"This example simulates reading accelerometer and gyroscope data and performing a basic complementary filter for orientation estimation (pitch and roll). Magnetometer data could be added for yaw."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport math\nimport time\n\nclass SimulatedIMU:\n    def __init__(self, dt=0.01):\n        self.dt = dt # Time step\n        self.pitch = 0.0\n        self.roll = 0.0\n        self.yaw = 0.0 # Not calculated in this simple example\n\n    def get_imu_data(self, true_pitch=0, true_roll=0, angular_velocity_z=0):\n        """Simulates accelerometer and gyroscope readings with some noise."""\n\n        # Simulate accelerometer (measures gravity components + linear acceleration)\n        # Assuming no linear acceleration for simplicity, only gravity component\n        # ax = g * sin(pitch)\n        # ay = -g * cos(pitch) * sin(roll)\n        # az = -g * cos(pitch) * cos(roll)\n        g = 9.81\n        accel_x = g * math.sin(math.radians(true_pitch)) + np.random.normal(0, 0.05)\n        accel_y = -g * math.cos(math.radians(true_pitch)) * math.sin(math.radians(true_roll)) + np.random.normal(0, 0.05)\n        accel_z = -g * math.cos(math.radians(true_pitch)) * math.cos(math.radians(true_roll)) + np.random.normal(0, 0.05)\n\n        # Simulate gyroscope (measures angular velocity)\n        # Assuming only pitch and roll changes for this example\n        gyro_x = -math.radians(angular_velocity_z) * math.sin(math.radians(true_roll)) + np.random.normal(0, 0.01) # Example, simplify\n        gyro_y = math.radians(angular_velocity_z) * math.cos(math.radians(true_roll)) + np.random.normal(0, 0.01) # Example, simplify\n        gyro_z = math.radians(angular_velocity_z) + np.random.normal(0, 0.01) # Angular velocity around Z (yaw)\n\n        return np.array([accel_x, accel_y, accel_z]), np.array([gyro_x, gyro_y, gyro_z])\n\n    def complementary_filter(self, accel_data, gyro_data, alpha=0.98):\n        """\n        Estimates pitch and roll using a complementary filter.\n        alpha: weight for gyroscope (1-alpha for accelerometer).\n        """\n        # Gyroscope integration (high-pass filter)\n        self.roll += gyro_data[0] * self.dt # Angular velocity around X\n        self.pitch += gyro_data[1] * self.dt # Angular velocity around Y\n\n        # Accelerometer estimation (low-pass filter)\n        # Avoid division by zero when calculating roll_accel and pitch_accel\n        if accel_data[2] != 0:\n            pitch_accel = math.atan2(accel_data[0], math.sqrt(accel_data[1]**2 + accel_data[2]**2))\n            roll_accel = math.atan2(accel_data[1], accel_data[2])\n        else:\n            pitch_accel = self.pitch # Maintain previous if Z-accel is zero\n            roll_accel = self.roll # Maintain previous if Z-accel is zero\n\n        # Combine using complementary filter\n        self.pitch = alpha * self.pitch + (1 - alpha) * pitch_accel\n        self.roll = alpha * self.roll + (1 - alpha) * roll_accel\n\n        return math.degrees(self.pitch), math.degrees(self.roll)\n\n# --- Usage ---\nif __name__ == "__main__":\n    imu = SimulatedIMU(dt=0.01)\n\n    print("Time | True Pitch | True Roll | Est. Pitch | Est. Roll")\n    print("-----------------------------------------------------")\n\n    for t in range(200): # Simulate for 2 seconds\n        true_p = 10 * math.sin(t * 0.1) # Oscillating pitch\n        true_r = 5 * math.cos(t * 0.05) # Oscillating roll\n        ang_vel_z = 0 # No yaw motion for simplicity\n\n        accel, gyro = imu.get_imu_data(true_pitch=true_p, true_roll=true_r, angular_velocity_z=ang_vel_z)\n        est_pitch, est_roll = imu.complementary_filter(accel, gyro)\n\n        if t % 20 == 0: # Print every 0.2 seconds\n            print(f"{t*imu.dt:.2f}s | {true_p:.2f}\xb0 | {true_r:.2f}\xb0 | {est_pitch:.2f}\xb0 | {est_roll:.2f}\xb0")\n        time.sleep(imu.dt)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"5-exercises",children:"5. Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LiDAR Data Filtering:"})," Modify the ",(0,t.jsx)(n.code,{children:"SimulatedLiDAR"})," example to include a simple median filter to reduce noise in the ",(0,t.jsx)(n.code,{children:"scan_data"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"IMU Data Visualization:"})," For the ",(0,t.jsx)(n.code,{children:"SimulatedIMU"})," example, suggest a method to visualize the estimated pitch and roll over time (e.g., using a plotting library like ",(0,t.jsx)(n.code,{children:"matplotlib"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Fusion Discussion:"})," Research and briefly describe another sensor fusion technique (besides the complementary filter) that could be used to combine IMU data with GPS or vision data for more robust position and orientation estimation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Embodied AI Scenario:"})," Propose a scenario where embodied intelligence is critical for a Physical AI agent to succeed, and explain why a disembodied AI would struggle in that scenario."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Camera vs. LiDAR for Autonomous Driving:"})," Discuss the pros and cons of using primarily cameras versus primarily LiDAR for perception in an autonomous driving system. How might they complement each other?"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"6-short-quiz",children:"6. Short Quiz"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Which of the following is NOT a core aspect of embodied intelligence?\na) Situatedness\nb) Interaction with the environment\nc) Purely abstract reasoning\nd) Morphological computation"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What type of sensor uses pulsed laser light to generate precise 3D point clouds?\na) IMU\nb) Ultrasonic sensor\nc) RGB-D Camera\nd) LiDAR"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"An IMU typically combines measurements from which three components?\na) Accelerometer, Gyroscope, Barometer\nb) Accelerometer, Gyroscope, Magnetometer\nc) Camera, Gyroscope, Accelerometer\nd) Magnetometer, LiDAR, Accelerometer"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["In the ",(0,t.jsx)(n.code,{children:"SimulatedIMU"})," example, the complementary filter's ",(0,t.jsx)(n.code,{children:"alpha"})," parameter is a weight for which sensor?\na) Accelerometer\nb) Gyroscope\nc) Magnetometer\nd) GPS"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"True or False: Physical AI agents primarily operate in simulated environments without direct physical interaction.\na) True\nb) False"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Quiz Answers:"})," 1. c, 2. d, 3. b, 4. b, 5. b"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var s=i(6540);const t={},a=s.createContext(t);function r(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);